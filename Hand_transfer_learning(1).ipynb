{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6aKC2kzyo7u",
        "outputId": "e8437b28-727a-4d40-a0d3-a6bb62c7a513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-19 16:50:14--  http://dl.deepnn.ir/Diverse_hand_gesture_dataset.zip\n",
            "Resolving dl.deepnn.ir (dl.deepnn.ir)... 188.40.164.149\n",
            "Connecting to dl.deepnn.ir (dl.deepnn.ir)|188.40.164.149|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1088794071 (1.0G) [application/zip]\n",
            "Saving to: ‘Diverse_hand_gesture_dataset.zip’\n",
            "\n",
            "Diverse_hand_gestur 100%[===================>]   1.01G  18.6MB/s    in 56s     \n",
            "\n",
            "2024-01-19 16:51:12 (18.5 MB/s) - ‘Diverse_hand_gesture_dataset.zip’ saved [1088794071/1088794071]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://dl.deepnn.ir/Diverse_hand_gesture_dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q Diverse_hand_gesture_dataset.zip"
      ],
      "metadata": {
        "id": "AA2nbzoPy2Lr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Set the path to your directory containing PNG and TXT files\n",
        "input_directory = '/content/train'\n",
        "\n",
        "# Create a directory to store the organized files\n",
        "output_directory = 'organized_train'\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "validation_dir = \"validation\"\n",
        "\n",
        "# Define a dictionary to map numeric class numbers to strings\n",
        "class_mapping = {\n",
        "    0: 'zero',\n",
        "    1: 'dislike',\n",
        "    2: 'five',\n",
        "    3: 'exactly',\n",
        "    4: 'two',\n",
        "    5: 'three',\n",
        "    6: 'left',\n",
        "    7: 'like'\n",
        "}\n",
        "\n",
        "num = 0\n",
        "# 10% validation\n",
        "\n",
        "# Loop through each TXT file in the input directory\n",
        "for txt_file in os.listdir(input_directory):\n",
        "    if txt_file.endswith('.txt'):\n",
        "        txt_path = os.path.join(input_directory, txt_file)\n",
        "\n",
        "        # Read the class information from the first column of the TXT file\n",
        "        with open(txt_path, 'r') as file:\n",
        "            class_number = int(file.readline().strip().split()[0])\n",
        "\n",
        "        # Map the numeric class number to the corresponding string\n",
        "        class_name = class_mapping.get(class_number, 'unknown')\n",
        "\n",
        "        # Create a directory for the class if it doesn't exist\n",
        "        if(num%10 == 0):\n",
        "          class_directory = os.path.join(validation_dir, class_name)\n",
        "          os.makedirs(class_directory, exist_ok=True)\n",
        "        else:\n",
        "          class_directory = os.path.join(output_directory, class_name)\n",
        "          os.makedirs(class_directory, exist_ok=True)\n",
        "\n",
        "        # Copy the corresponding PNG file to the class directory\n",
        "        png_file = os.path.splitext(txt_file)[0] + '.png'\n",
        "        png_path = os.path.join(input_directory, png_file)\n",
        "        shutil.copy(png_path, class_directory)\n",
        "        num += 1"
      ],
      "metadata": {
        "id": "-B94bhIMy2uz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications import VGG16\n",
        "\n",
        "images_size = 224"
      ],
      "metadata": {
        "id": "uUT9jX3wy3xD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conv_base = VGG16(weights='imagenet',\n",
        "                  include_top=False,\n",
        "                  input_shape=(images_size, images_size, 3))\n",
        "\n",
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "    if layer.name == 'block5_conv1':\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "metadata": {
        "id": "dhhAAtUCzDcR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    rotation_range = 20,\n",
        "    width_shift_range = 0.2,\n",
        "    height_shift_range = 0.2,\n",
        "    shear_range = 0.2,\n",
        "    zoom_range = 0.2,\n",
        "    brightness_range=[0.4, 1.0],\n",
        "    horizontal_flip = True,\n",
        "    fill_mode = 'nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    rescale = 1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\n",
        "    output_directory,\n",
        "    # All images will be resized to 150x150\n",
        "    target_size = (images_size, images_size),\n",
        "    batch_size = 96,\n",
        "    # Since we use binary_crossentropy loss, we need binary labels\n",
        "    class_mode = 'sparse')\n",
        "\n",
        "valid_gen = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size = (images_size, images_size),\n",
        "    batch_size = 40,\n",
        "    class_mode = 'sparse')\n",
        "\n",
        "test_gen = test_datagen.flow_from_directory(\n",
        "    \"test\",\n",
        "    target_size = (images_size, images_size),\n",
        "    batch_size = 20,\n",
        "    class_mode = 'sparse')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZf6VI_41SBR",
        "outputId": "0c87613b-b9ba-42d3-f39c-524b458dc904"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5760 images belonging to 8 classes.\n",
            "Found 640 images belonging to 8 classes.\n",
            "Found 1590 images belonging to 8 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(8, activation='softmax'))\n",
        "\n",
        "model.compile(loss= keras.losses.SparseCategoricalCrossentropy(),\n",
        "              optimizer=optimizers.RMSprop(learning_rate=0.0001),\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.fit(train_gen,\n",
        "            epochs=30,\n",
        "            validation_data = valid_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB8rrD3HzGxi",
        "outputId": "74c000d7-409d-4ed8-8a15-8978875b6853"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "60/60 [==============================] - 102s 2s/step - loss: 1.8533 - acc: 0.2790 - val_loss: 1.3180 - val_acc: 0.5156\n",
            "Epoch 2/30\n",
            "60/60 [==============================] - 102s 2s/step - loss: 1.2870 - acc: 0.5300 - val_loss: 0.6080 - val_acc: 0.8094\n",
            "Epoch 3/30\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.9199 - acc: 0.6760 - val_loss: 0.5876 - val_acc: 0.7719\n",
            "Epoch 4/30\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.6813 - acc: 0.7566 - val_loss: 0.3520 - val_acc: 0.8781\n",
            "Epoch 5/30\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.5669 - acc: 0.8066 - val_loss: 0.1452 - val_acc: 0.9438\n",
            "Epoch 6/30\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.4482 - acc: 0.8425 - val_loss: 0.4082 - val_acc: 0.8828\n",
            "Epoch 7/30\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.3962 - acc: 0.8601 - val_loss: 0.1142 - val_acc: 0.9500\n",
            "Epoch 8/30\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.3422 - acc: 0.8780 - val_loss: 0.1729 - val_acc: 0.9344\n",
            "Epoch 9/30\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.3013 - acc: 0.8977 - val_loss: 0.0702 - val_acc: 0.9734\n",
            "Epoch 10/30\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2691 - acc: 0.9120 - val_loss: 0.0662 - val_acc: 0.9734\n",
            "Epoch 11/30\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2499 - acc: 0.9153 - val_loss: 0.0603 - val_acc: 0.9828\n",
            "Epoch 12/30\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.2187 - acc: 0.9220 - val_loss: 0.0614 - val_acc: 0.9797\n",
            "Epoch 13/30\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.1869 - acc: 0.9370 - val_loss: 0.2399 - val_acc: 0.9281\n",
            "Epoch 14/30\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.1956 - acc: 0.9340 - val_loss: 0.1348 - val_acc: 0.9516\n",
            "Epoch 15/30\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.1745 - acc: 0.9420 - val_loss: 0.0438 - val_acc: 0.9859\n",
            "Epoch 16/30\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.1633 - acc: 0.9462 - val_loss: 0.0452 - val_acc: 0.9828\n",
            "Epoch 17/30\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.1527 - acc: 0.9512 - val_loss: 0.0721 - val_acc: 0.9781\n",
            "Epoch 18/30\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.1280 - acc: 0.9530 - val_loss: 0.0642 - val_acc: 0.9766\n",
            "Epoch 19/30\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.1273 - acc: 0.9571 - val_loss: 0.0350 - val_acc: 0.9875\n",
            "Epoch 20/30\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.1279 - acc: 0.9568 - val_loss: 0.0509 - val_acc: 0.9859\n",
            "Epoch 21/30\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.1158 - acc: 0.9604 - val_loss: 0.0210 - val_acc: 0.9891\n",
            "Epoch 22/30\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.1153 - acc: 0.9606 - val_loss: 0.0399 - val_acc: 0.9844\n",
            "Epoch 23/30\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.1084 - acc: 0.9642 - val_loss: 0.0291 - val_acc: 0.9891\n",
            "Epoch 24/30\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.0934 - acc: 0.9707 - val_loss: 0.0701 - val_acc: 0.9781\n",
            "Epoch 25/30\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.0980 - acc: 0.9665 - val_loss: 0.4595 - val_acc: 0.9141\n",
            "Epoch 26/30\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.0884 - acc: 0.9694 - val_loss: 0.4074 - val_acc: 0.9125\n",
            "Epoch 27/30\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.0981 - acc: 0.9691 - val_loss: 0.0196 - val_acc: 0.9937\n",
            "Epoch 28/30\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.0868 - acc: 0.9708 - val_loss: 0.0370 - val_acc: 0.9891\n",
            "Epoch 29/30\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.0880 - acc: 0.9726 - val_loss: 0.0351 - val_acc: 0.9906\n",
            "Epoch 30/30\n",
            "60/60 [==============================] - 109s 2s/step - loss: 0.0871 - acc: 0.9724 - val_loss: 0.0486 - val_acc: 0.9875\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7aa286972fb0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.evaluate(test_gen)"
      ],
      "metadata": {
        "id": "faRK-6d1zJRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e251eaab-d321-4bbf-8942-71a8c3f72f6c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 16s 196ms/step - loss: 0.9212 - acc: 0.8585\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9211513996124268, 0.8584905862808228]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights(\"Model.keras\")"
      ],
      "metadata": {
        "id": "oz8JXvw3eZ2I"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}